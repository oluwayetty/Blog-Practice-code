{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space_size : 6\n",
      "state_space_size : 128\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"SpaceInvaders-ram-v0\")\n",
    "\n",
    "action_space_size = env.action_space.n # number of actions in the environment\n",
    "state_space_size = env.observation_space.shape[0] # number of states in the environment\n",
    "\n",
    "print(\"action_space_size :\", action_space_size) #left, right, up, down.\n",
    "print(\"state_space_size :\", state_space_size)\n",
    "\n",
    "'''\n",
    "initialize Q_table\n",
    "rows represents the states\n",
    "columns represents the actions\n",
    "'''\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "# print(q_table) #returns table of 4rows by 16 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning parameters\n",
    "num_episodes = 10000 #num of episodes agents plays during training\n",
    "'''\n",
    "max num of steps to take in each episodeself.\n",
    "If agent take the 100 steps wihout reaching goal,\n",
    "game is terminated.\n",
    "'''\n",
    "max_steps_per_episode = 1000\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001 #or 0.01\n",
    "\n",
    "all_episodes_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 241 is out of bounds for axis 0 with size 128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5c1ad774d2b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# update Q-table for Q(s,a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mold_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mlearned_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiscount_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 241 is out of bounds for axis 0 with size 128"
     ]
    }
   ],
   "source": [
    "#Q-Learning algorithm\n",
    "for each_episode in range(num_episodes):\n",
    "    state = env.reset() # starting state of game\n",
    "\n",
    "    done = False # value to express the game is not finished yet\n",
    "    current_episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        #exploration-exploitation trade off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            # choose action that has the high Q value for that state\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            # choose an action randomly\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        '''\n",
    "        step function initiates the action we choose from the conditional statement above\n",
    "        step also returns a tuple of the following:\n",
    "        new_state => the new state the agent is, as a result of the action taken in line 64\n",
    "        reward => reward of the action agent took\n",
    "        done => A boolean of whether the action ended our episode/agent is done with the episode\n",
    "        info => diagnostics information about our environment for debugging purpose\n",
    "        '''\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update Q-table for Q(s,a)\n",
    "        old_q_value = (1 - learning_rate) * q_table[state,action]\n",
    "        learned_q_value = learning_rate * (reward + (discount_rate * np.max(q_table[new_state,:])))\n",
    "        # import pdb; pdb.set_trace()\n",
    "        # q_table[state,action] = q_table[state,action] * (1 - learning_rate) + learning_rate * \\\n",
    "        #                         (reward + discount_rate * np.max(q_table[new_state,:]))\n",
    "        q_table[state,action] = old_q_value + learned_q_value\n",
    "\n",
    "        state = new_state\n",
    "        current_episode_reward += reward\n",
    "\n",
    "        # check if we are done i.e agent steps in a hole or reached the goal\n",
    "        if done == True:\n",
    "            print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "            break; #end the game\n",
    "\n",
    "    # once an episode is finished\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*each_episode)\n",
    "    all_episodes_rewards.append(current_episode_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average reward per a number of eposides\n",
    "EPISODES = 1000\n",
    "rewards_per_hundred_episodes = np.split(np.array(all_episodes_rewards), num_episodes/1000)\n",
    "count = 1000\n",
    "print(\"*********Average reward per \", 1000 ,\" episodes ***********\\n\")\n",
    "for r in rewards_per_hundred_episodes:\n",
    "    # import pdb; pdb.set_trace()\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "\n",
    "# updated Q_table\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action\n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # Choose action with highest Q-value for current state\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
